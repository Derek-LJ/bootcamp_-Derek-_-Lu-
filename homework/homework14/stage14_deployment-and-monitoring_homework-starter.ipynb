{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "509a2150",
   "metadata": {},
   "source": [
    "# Homework Starter — Stage 14: Deployment & Monitoring\n",
    "\n",
    "Use this template to draft your reflection and (optionally) sketch a dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae655fdc",
   "metadata": {},
   "source": [
    "## 1) Reflection (200–300 words)\n",
    "- Deploying a machine learning model into production introduces several risks across the data, model, system, and business layers. From a **data perspective**, issues such as missing values, delayed data pipelines, or schema changes can cause unexpected failures. For instance, if input features shift distribution due to market changes (data drift), predictions may become unreliable without immediate detection. At the **model level**, staleness is a key risk: a model trained once may degrade in performance as patterns evolve, leading to higher error rates or poorly calibrated probabilities. Regular retraining schedules and rolling performance checks are essential.\n",
    "\n",
    "- At the **system level**, latency and reliability must be monitored closely. For example, if p95 latency exceeds 250ms or error rates spike above 2%, an on-call notification should be triggered. These thresholds ensure user experience remains consistent and reliable. From a **business perspective**, misaligned predictions can result in significant losses. For example, in a loan approval scenario, an approval rate that suddenly deviates from historical norms or a rising bad rate could directly impact revenue and customer trust.\n",
    "\n",
    "- Clear ownership is critical. The ML engineering team may be responsible for monitoring system metrics and responding to alerts, while the data science team tracks model drift and calibration errors. Business stakeholders should review approval and bad rates regularly. Establishing a defined handoff process ensures that when a threshold is breached, the right team is notified, root causes are investigated, and corrective actions such as data fixes, retraining, or rollback are executed promptly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0163c438",
   "metadata": {},
   "source": [
    "## 2) Optional: Dashboard Sketch\n",
    "A monitoring dashboard could contain four main panels:\n",
    "\n",
    "- **Data Layer:** Freshness (minutes since last update), null rates, schema hash checks.\n",
    "- **Model Layer:** Rolling MAE/AUC, calibration error.\n",
    "- **System Layer:** p95 latency, error rate trends.\n",
    "- **Business Layer:** Approval rate, bad rate, revenue impact indicators.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional helper: simple structure to list metrics\n",
    "monitoring = {\n",
    "    'data': ['freshness_minutes', 'null_rate', 'schema_hash'],\n",
    "    'model': ['rolling_mae_or_auc', 'calibration_error'],\n",
    "    'system': ['p95_latency_ms', 'error_rate'],\n",
    "    'business': ['approval_rate', 'bad_rate']\n",
    "}\n",
    "monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
